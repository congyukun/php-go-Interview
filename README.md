# GO相关



## 进程 线程 协程

- 进程
  即运行起来的程序, 每个进程都有自己的独立内存空间，拥有自己独立的地址空间、独立的堆和栈，既不共享堆，亦不共享栈。一个程序至少有一个进程，一个进程至少有一个线程。进程切换只发生在内核态，是系统资源分配的最小单位
- 线程
  线程拥有自己独立的栈和共享的堆，共享堆，不共享栈，是由操作系统调度，是操作系统调度（CPU调度）执行的最小单位。对于进程和线程，都是由内核进行调度，有 CPU 时间片的概念, 进行抢占式调度。内核由系统内核进行调度, 系统为了实现并发,会不断地切换线程执行, 由此会带来线程的上下文切换，一个进程可以有多个线程，每个线程会共享父进程的资源
- 协程
  独立的栈空间, 共享堆空间，协程是由程序员在协程的代码中显示调度，属于用户态线程，协程是对内核透明的, 也就是系统完全不知道有协程的存在, 完全由用户自己的程序进行调度。在栈大小分配方便，且每个协程占用的默认占用内存很小，只有 2kb ，而线程需要 8mb，相较于线程,因为协程是对内核透明的，所以栈空间大小可以按需增大减小，轻量且开销较小。

## 解释GMP模型

GMP 模型是 Go 语言调度器采用的并发编程模型，它包含三个重要的组件：Goroutine（G）、逻辑处理器（P）和操作系统线程（M）。这些组件协同工作以实现 Go 程序的高效并发执行。

具体来说，

- Goroutine (G) 是 Go 语言中轻量级的并发执行单元，类似于线程但比线程更小、更灵活。每个 goroutine 都有自己独立的堆栈和寄存器等信息，可以通过 go 关键字创建并发执行任务。
- 逻辑处理器（P）是一个虚拟的执行单元，负责调度 goroutine 和执行 Go 代码。Go 程序中有多个 P，每个 P 可以运行多个 goroutine，因此可以实现真正的并发执行。
- 操作系统线程（M）是实际的执行单元，负责将 goroutine 调度到逻辑处理器上执行。Go 程序中通常会创建多个 M，以便在多核 CPU 上实现并发执行。

## GMP的调度流程

GMP 调度器采用抢占式的协作调度，具体调度流程如下：

1. 主线程启动，在主线程中创建一个操作系统线程（M）和一个逻辑处理器（P）。
2. 当有 goroutine 函数被调用时，它会被放入到一个全局队列中等待执行。
3. P 从全局队列中获取任务并执行。如果 P 执行的 goroutine 阻塞（例如在等待 I/O 完成），则该 P 的所有 goroutine 都会被暂停，P 会将自己标记为阻塞状态并开始寻找其他可用的 P。
4. 如果没有可用的 P，则 M 变为自由线程，并且会去创建一个新的 P，以便执行未完成的 goroutine。新的 P 将加入到一个全局 P 列表中，而 M 将继续尝试在列表中寻找可用的 P。
5. 当 goroutine 阻塞时，Goroutine 在堆上分配一块内存来保存其状态，并被添加到相关的等待队列中。而主线程会进入休眠状态，等待唤醒事件发生。
6. 当阻塞的 goroutine 可以继续执行时，调度器会将它从等待队列中移除，并将其重新添加到全局队列中，等待 P 来执行。
7. 当程序结束时，所有未完成的 goroutine 都会被杀死，而 P 和 M 也会被回收。

## P和M的个数

- P: 由启动时环境变量 `$GOMAXPROCS` 或者是由 `runtime`的方法`GOMAXPROCS()`决定。这意味着在程序执行的任意时刻都只有`$GOMAXPROCS`个goroutine在同时运行。
- M:
  - Go 语言本身的限制：Go 程序启动时，会设置 M 的最大数量，默认 10000，但是内核很难支持这么多的线程数，所以这个限制可以忽略。
  - runtime/debug 中的 SetMaxThreads 函数，设置 M 的最大数量。
  - 一个 M 阻塞了，会创建新的 M。

M 与 P 的数量没有绝对关系，一个 M 阻塞，P 就会去创建或者切换另一个 M，所以，即使 P 的默认数量是 1，也有可能会创建很多个 M 出来。

## **P和M何时会被创建**

P: 在确定了 P 的最大数量 n 后，运行时系统会根据这个数量创建 n 个 P。

M: 没有足够的 M 来关联 P 并运行其中的可运行的 G 时创建。比如所有的 M 此时都阻塞住了，而 P 中还有很多就绪任务，就会去寻找空闲的 M，而没有空闲的，就会去创建新的 M。



## goroutine创建流程

Go 语言中，goroutine 的创建非常简单，只需要使用 `go` 关键字即可。`go` 关键字后面跟随一个函数调用（或者匿名函数），该函数就会在新的 goroutine 中并发执行。

具体的创建流程如下：

1. 确定要并发执行的函数或匿名函数。
2. 使用 `go` 关键字创建新的 goroutine 并启动并发执行。
3. 调度器将新的 goroutine 分配给逻辑处理器，然后绑定到操作系统线程上执行。
4. 新的 goroutine 开始执行，其状态和堆栈等信息由调度器自动管理。
5. 当函数执行结束时，goroutine 自动终止，其状态和堆栈等资源也会被回收。

值得注意的是，Go 语言的调度器可以同时运行数百万个 goroutine，因此不必担心创建大量的 goroutine 会导致系统负荷过重。但是，如果你的程序中存在频繁创建和销毁 goroutine 的情况，应该考虑使用 sync.Pool 等技术来优化内存分配和回收效率。

## goroutine什么时候会被挂起

goroutine 会在以下情况下被挂起：

1. 发生阻塞，例如等待 I/O 操作的完成或者发送或接收通道上的数据时没有可用的对等方。
2. 发生调用 runtime.Gosched()，让出 CPU 给其他 goroutine 执行。
3. 发生同步操作，例如 sync.Mutex 或 sync.WaitGroup 的锁定和解锁操作。
4. 发生垃圾回收（GC）。
5. 发生错误，例如 panic 或者超时。

## 同时启动了一万个goroutine，会如何调度

一万个G会按照P的设定个数，尽量平均地分配到每个P的本地队列中。如果所有本地队列都满了，那么剩余的G则会分配到GMP的全局队列上。接下来便开始执行GMP模型的调度策略：

- **本地队列轮转**：每个P维护着一个包含G的队列，不考虑G进入系统调用或IO操作的情况下，P周期性的将G调度到M中执行，执行一小段时间，将上下文保存下来，然后将G放到队列尾部，然后从队首中重新取出一个G进行调度。
- **系统调用**：P的个数默认等于CPU核数，每个M必须持有一个P才可以执行G，一般情况下M的个数会略大于P的个数，这多出来的M将会在G产生系统调用时发挥作用。当该G即将进入系统调用时，对应的M由于陷入系统调用而进被阻塞，将释放P，进而某个空闲的M1获取P，继续执行P队列中剩下的G。
- **工作量窃取**：多个P中维护的G队列有可能是不均衡的，当某个P已经将G全部执行完，然后去查询全局队列，全局队列中也没有新的G，而另一个M中队列中还有3很多G待运行。此时，空闲的P会将其他P中的G偷取一部分过来，一般每次偷取一半。

## goroutine内存泄漏原因和处理

**原因**：

Goroutine 是轻量级线程，需要维护执行用户代码的上下文信息。在运行过程中也需要消耗一定的内存来保存这类信息，而这些内存在目前版本的 Go 中是不会被释放的。因此，如果一个程序持续不断地产生新的 goroutine、且不结束已经创建的 goroutine 并复用这部分内存，就会造成内存泄漏的现象。造成泄露的大多数原因有以下三种：

- Goroutine 内正在进行 channel/mutex 等读写操作，但由于逻辑问题，某些情况下会被一直阻塞。
- Goroutine 内的业务逻辑进入死循环，资源一直无法释放。
- Goroutine 内的业务逻辑进入长时间等待，有不断新增的 Goroutine 进入等待。

**解决方法**：

- 使用channel
  - 1、使用channel接收业务完成的通知
  - 2、业务执行阻塞超过设定的超时时间，就会触发超时退出
- 使用pprof排查
  - pprof是由 Go 官方提供的可用于收集程序运行时报告的工具，其中包含 CPU、内存等信息。当然，也可以获取运行时 goroutine 堆栈信息。
- 使用 context 进行超时控制：context 包提供了超时控制等功能，可以避免 goroutine 在执行过程中发生死循环等问题。
- 利用 Go 的垃圾回收机制：Go 运行时系统包含了自动垃圾回收机制，可以周期性地检查并回收不再使用的内存。

## Go语言中的并发安全性是什么？如何确保并发安全性？

并发安全性是指在并发编程中，多个goroutine对共享资源的访问不会导致数据竞争和不确定的结果。

为了确保并发安全性，可以采取以下措施：

- 使用互斥锁（Mutex）：通过使用互斥锁来保护共享资源的访问，一次只允许一个goroutine访问共享资源，从而避免竞争条件。
- 使用原子操作（Atomic Operations）：对于简单的读写操作，可以使用原子操作来保证操作的原子性，避免竞争条件。
- 使用通道（Channel）：通过使用通道来进行goroutine之间的通信和同步，避免共享资源的直接访问。
- 使用同步机制：使用同步机制如等待组（WaitGroup）、条件变量（Cond）等来协调多个goroutine的执行顺序和状态。

通过以上措施，可以确保并发程序的安全性，避免数据竞争和不确定的结果。



## Go语言中的defer关键字有什么作用？

defer关键字用于延迟函数的执行，即在函数退出前执行某个操作。defer通常用于释放资源、关闭文件、解锁互斥锁等清理操作，以确保在函数执行完毕后进行处理。

也可以使用defer语句结合time包实现函数执行时间的统计。

## 什么是互斥锁（Mutex）？在Go语言中如何使用互斥锁来保护共享资源？

互斥锁是一种并发编程中常用的同步机制，用于保护共享资源的访问。

在Go语言中，可以使用sync包中的Mutex类型来实现互斥锁。通过调用Lock方法来获取锁，保护共享资源的访问，然后在使用完共享资源后调用Unlock方法释放锁。

``` go
我们定义了一个全局变量counter和一个sync.Mutex类型的互斥锁mutex。在increment函数中，我们使用mutex.Lock()获取锁，对counter进行递增操作，然后使用mutex.Unlock()释放锁。通过使用互斥锁，我们确保了对counter的并发访问的安全性。

package main

import (
 "fmt"
 "sync"
)
var (
 counter int
 mutex   sync.Mutex
)
func increment() {
 mutex.Lock()
 counter++
 mutex.Unlock()
}
func main() {
 var wg sync.WaitGroup
 for i := 0; i < 1000; i++ {
  wg.Add(1)
  go func() {
   defer wg.Done()
   increment()
  }()
 }
 wg.Wait()
 fmt.Println("Counter:", counter)
}

```

## 原子操作和锁的区别是什么？

原子操作和锁是并发编程中常用的两种同步机制，它们的区别如下：

1. 作用范围：

- 原子操作（Atomic Operations）：原子操作是一种基本的操作，可以在单个指令级别上执行，保证操作的原子性。原子操作通常用于对共享变量进行读取、写入或修改等操作，以确保操作的完整性。
- 锁（Lock）：锁是一种更高级别的同步机制，用于保护临界区（Critical Section）的访问。锁可以用于限制对共享资源的并发访问，以确保线程安全。

1. 使用方式：

- 原子操作：原子操作是通过硬件指令或特定的原子操作函数来实现的，可以直接应用于变量或内存位置，而无需额外的代码。
- 锁：锁是通过编程语言提供的锁机制来实现的，需要显式地使用锁的相关方法或语句来保护临界区的访问。

1. 粒度：

- 原子操作：原子操作通常是针对单个变量或内存位置的操作，可以在非常细粒度的层面上实现同步。
- 锁：锁通常是针对一段代码或一组操作的访问进行同步，可以控制更大粒度的临界区。

1. 性能开销：

- 原子操作：原子操作通常具有较低的性能开销，因为它们是在硬件级别上实现的，无需额外的同步机制。
- 锁：锁通常具有较高的性能开销，因为它们需要进行上下文切换和线程同步等操作。

综上所述，**原子操作和锁是两种不同的同步机制，用于处理并发编程中的同步问题。原子操作适用于对单个变量的读写操作，具有较低的性能开销。而锁适用于对一段代码或一组操作的访问进行同步，具有更高的性能开销。选择使用原子操作还是锁取决于具体的场景和需求。**

需要注意的是，**原子操作通常用于对共享变量进行简单的读写操作，而锁更适用于对临界区的访问进行复杂的操作和保护。在设计并发程序时，需要根据具体的需求和性能要求来选择合适的同步机制。**



## Go语言中的select语句是什么？请给出一个使用select语句的示例?

select语句是Go语言中用于处理通道操作的一种机制。它可以同时监听多个通道的读写操作，并在其中任意一个通道就绪时执行相应的操作。

## slice扩容策略

**Go 1.18版本后**

新申请的容量如果大于当前容量的两倍，会将新申请的容量直接作为新的容量，如果新申请的容量小于当前容量的两倍，会有一个阈值，即当前切片容量小于1024时，切片会将当前容量的2倍作为新申请的容量，如果大于等于1024，会将当前的容量的1.25倍作为新申请的容量。

Go 1.18版本后**

新申请的容量如果大于当前容量的两倍，会将新申请的容量直接作为新的容量，如果新申请的容量小于当前容量的两倍，会有一个阈值，即当前切片容量小于256时，切片会将当前容量的2倍作为新申请的容量，如果大于等于256，会将当前的容量的1.25倍+192作为新申请的容量，扩容的时候更加平滑，不会出现从2到1.25的突变。

## Go 的垃圾回收机制

- 手动释放占用的内存空间
  程序代码中也可以使用runtime.GC()来手动触发GC

- 自动内存回收
  内存分配量达到阀值触发GC
  定期触发GC, 默认情况下，最长2分钟触发一次GC

- 三色标记最大的好处是可以异步执行，以中断时间极少的代价或者完全没有中断来进行整个 GC。
  Go 采用的是三色标记法，将内存里的对象分为了三种：

  - 白色对象：未被使用的对象；
  - 灰色对象：当前对象有引用对象，但是还没有对引用对象继续扫描过；
  - 黑色对象，对灰色对象的引用对象已经全部扫描过了，下次不用再扫描它了。

  只要是新创建的对象, 默认都会标记为白色, 当垃圾回收开始时，Go 会把根对象标记为灰色，其他对象标记为白色，然后从根对象遍历搜索，按照上面的定义去不断的对灰色对象进行扫描标记。当没有灰色对象时，表示所有对象已扫描过，然后就可以开始清除白色对象了。

## channel 的内部实现是怎么样的

底层 hchan结构体的主要组成部分

- 用来保存goroutine之间传递数据的循环链表-------->buf
- 用来记录此循环链表当前发送或接收数据的下标值---------->sendx和recvx
- 用于保存向该chan发送和从该chan接收数据的goroutine队列---------->sendq和recvq
- 保证chan写入和读取数据时的线程安全的锁----------->lock

channel 内部通过队列实现, 有一个唤醒队列队列作为缓冲区，队列的长度是创建chan时指定的。维护了两个 goroutine 等待队列，一个是待发送数据的 goroutine 队列，另一个是待读取数据的 goroutine 队列。
从channel中读数据，如果channel缓冲区为空或者没有缓冲区，当前goroutine会被阻塞；向channel中写数据，如果channel缓冲区已满或者没有缓冲区，当前goroutine会被阻塞。被阻塞的goroutine将会被挂在channel的等待队列中：

- 因读阻塞的goroutine会被向channel写入数据的goroutine唤醒
- 因写阻塞的goroutine会被从channel读数据的goroutine唤醒

直到有其他 goroutine 执行了与之相反的读写操作，将它重新唤起。

并且内部维护了一个互斥锁, 来保证线程安全, 即在对buf中的数据进行入队和出队操作时, 为当前channel使用了互斥锁, 防止多个线程并发修改数据

向channel写数据
![在这里插入图片描述](https://img-blog.csdnimg.cn/dd86e9e50bd843dbacd09407fc75c292.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ0hBTzkxNzI=,size_20,color_FFFFFF,t_70,g_se,x_16)

1. 如果recvq队列不为空，说明缓冲区没有数据或无缓冲区，且有等待取值的goroutine在排队, 此时直接从recvq等待队列中取出一个G，并把数据写入，最后把该G唤醒，结束发送过程；
2. 如果缓冲区有空余位置，则把数据写入缓冲区中，结束发送过程；
3. 如果缓冲区没有空余位置，将当前G加入sendq队列，进入休眠，等待被读goroutine唤醒；

从channel读数据
![在这里插入图片描述](https://img-blog.csdnimg.cn/8774d2f487174c34bed217cceea66bc5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ0hBTzkxNzI=,size_20,color_FFFFFF,t_70,g_se,x_16)

1. 如果等待发送队列sendq不为空，且没有缓冲区，直接从sendq队列中取出G，把G中数据读出，最后把G唤醒，结束读取过程；
2. 如果等待发送队列sendq不为空，说明缓冲区已满，从缓冲队列中首部读取数据，从sendq等待发送队列中取出G，把G中的数据写入缓冲区尾部，结束读取过程；
3. 如果缓冲区中有数据，则从缓冲区取出数据，结束读取过程；

## 对已经关闭的 channel 进行读写，会怎么样

Go语言中的通道（channel）是一种非常有用的特性，用于在不同的goroutine之间传递数据, 在使用通道时，需要根据实际情况判断何时关闭通道，以避免出现不必要的竞态和内存泄漏。

- 当 channel 被关闭后，如果继续往里面写数据，程序会直接 panic 退出
- 关闭已经关闭的channel会发生Panic
- 关闭值为 nil 的channel会发生Panic
- 通道关闭后, 读取操作仍然可以从通道中读取到之前写入的数据。这是因为通道中的数据并没有立即消失，而是在读取完毕后被垃圾回收器回收。当关闭后的 channel 没有数据可读取时，将得到零值，即对应类型的默认值。

```go
// 判断当前 channel 是否被关闭
if v, ok := <-ch; !ok {
        fmt.Println("channel 已关闭，读取不到数据")
    }
1234
```

还可以使用下面的写法不断的获取 channel 里的数据：

```go
// range迭代从channel中读数据, 只有当channel关闭后才能退出循环, 否则没有数据了也会一直阻塞
   for data := range ch {
        // get data dosomething
    }
1234
```

使用for-range读取channel, 这样既安全又便利, 当channel关闭时, 循环会自动退出, 无需主动检测channel是否关闭, 可以防止读取已经关闭的channel, 造成读取数据为通道所存储类型的零值

## 并发安全性

并发安全性是指在并发编程中，多个goroutine对共享资源的访问不会导致数据竞争和不确定的结果。为了确保并发安全性，可以采取以下措施：

1. 使用互斥锁（Mutex）：通过使用互斥锁来保护共享资源的访问，一次只允许一个goroutine访问共享资源，从而避免竞争条件。
2. 使用原子操作（Atomic Operations）：对于简单的读写操作，可以使用原子操作来保证操作的原子性，避免竞争条件。
3. 使用通道（Channel）：通过使用通道来进行goroutine之间的通信和同步，避免共享资源的直接访问。
4. 使用同步机制：使用同步机制如等待组（WaitGroup）、条件变量（Cond）等来协调多个goroutine的执行顺序和状态。

## runtime

runtime包是Go语言的运行时系统，提供了与底层系统交互和控制的功能。它包含了与内存管理、垃圾回收、协程调度等相关的函数和变量

## 指针

指针是一种变量，存储了另一个变量的内存地址。通过指针，我们可以直接访问和修改变量的值，而不是对变量进行拷贝。

指针在传递大型数据结构和在函数间共享数据时非常有用。

## 接口 interface

Go语言中的接口（interface）是一种非常重要的特性，用于定义一组方法

接口是一种动态类型，它可以包含任何实现了它所定义的方法集的类型。在使用接口时，需要注意以下几点：

1. 接口是一种引用类型的数据结构，它的值可以为nil。
2. 实现接口的类型必须实现接口中所有的方法，否则会编译错误。
3. 接口的值可以赋给实现接口的类型的变量，反之亦然。
4. 在实现接口的类型的方法中，可以通过类型断言来判断接口值的实际类型和值。

## map 类型

map 是一种无序的键值对集合，也称为字典。map中的键必须是唯一的，而值可以重复。map 提供了快速的查找和插入操作，适用于需要根据键快速检索值的场景。
map 是使用哈希表、链表来实现的
我们从map中访问一个不存在的键时，它会返回该值类型的零值。
map是一种引用类型的数据结构，它的底层实现是一个哈希表。在使用map时，需要注意以下几点：

1. map是无序的，即元素的顺序不固定。每次迭代map的顺序可能不同
2. map的键必须是可以进行相等性比较的类型，如int、string、指针等。(通俗来说就是可以用 == 和 != 来比较的，除了slice、map、function这几个类型都可以)
3. map的值可以是任意类型，包括函数、结构体等。
4. map不是并发安全的, 在多个goroutine之间使用map时需要进行加锁，避免并发访问导致的竞态问题。

## map 为什么是不安全的

Go 官方认为 Go map 更应适配典型使用场景（不需要从多个 goroutine 中进行安全访问），而不是为了小部分情况（并发访问），导致大部分程序付出加锁代价（性能），所以决定了不支持并发安全。

因为它没有内置的锁机制来保护多个 goroutine 同时对其进行读写操作，而是会对某个标识位标记为 1，当多个 goroutine 同时对同一个 map 进行读写操作时，就会出现数据竞争和不一致的结果，当它检测到标识位为 1 时，将会直接 panic。

## 如何实现map线程安全

1. 使用 map + 读写锁 sync.RWMutex
2. 使用 sync.map
   sync.map是通过读写分离实现的，拿空间换时间, 通过冗余两个数据结构(read、dirty), 减少加锁对性能的影响, 可以无锁访问 read map, 而且会优先操作read map(不需要锁)，倘若只操作read map就可以满足要求(增删改查遍历)，那就不用去操作dirty map(它的读写都要加锁)，所以在某些特定场景中它发生锁竞争的频率会远远小于方式1。

sync.Map 适合读多写少的场景, 且性能比较好，否则并发性能很差, 因为会动态调整，miss次数多了之后，将dirty数据提升为read

concurrent-map 提供了一种高性能的解决方案:通过对内部 map 进行分片，降低锁粒度，从而达到最少的锁等待时间(锁冲突)。, double-checking, 延迟删除。 删除一个键值只是打标记，只有在提升dirty的时候才清理删除的数据

## map 的 key 为什么得是可比较类型的

map 的 key、value 是存在 buckets 数组里的，每个 bucket 又可以容纳 8 个 key-value 键值对。当要插入一个新的 key - value 时，会对 key 进行哈希计算得到一个 hash 值，然后根据 hash 值的低几位(取几位取决于桶的数量)来决定命中哪个 bucket。

在命中某个 bucket 后，又会根据 hash 值的高 8 位来决定是 8 个 key 里的哪个位置。若发生了 hash 冲突，即该位置上已经有其他 key 存在了，则会去其他空位置寻找插入。如果全都满了，则使用 overflow 指针指向一个新的 bucket，重复刚刚的寻找步骤。

从上面的流程可以看出，在判断 hash 冲突，即该位置是否已有其他 key 时，肯定是要进行比较的，所以 key 必须得是可比较类型的。像 slice、map、function 就不能作为 key。

## 遍历时, map 的 key 为什么是无序的

- 首先, map 在扩容后，会发生 key 的迁移，原来落在同一个 bucket 中的 key，可能迁移到别的 bucket 中。即使按顺序遍历 bucket，同时按顺序遍历 bucket 中的 key。由于扩容导致 key 的位置发生变化，遍历 map 也可能不按原来的顺序了
- 再者, 当遍历 map 时，并不是固定地从 0 号 bucket 开始遍历，每次都是从一个随机值序号的 bucket 开始遍历，并且是从这个 bucket 的一个随机序号的 cell 开始遍历。这样，即使你是一个写死的 map，仅仅只是遍历它，也不太可能会返回一个固定序列的 key/value 对了

## map的有序遍历

在Go语言中，map是无序的，每次迭代map的顺序可能不同。如果需要按特定顺序遍历map，可以采用以下步骤：

1. 创建一个切片来保存map的键。
2. 遍历map，将键存储到切片中。
3. 对切片进行排序。
4. 根据排序后的键顺序，遍历map并访问对应的值。

## make 和 new 的区别

**new**

- 分配内存。将会申请某个类型的内存, 内存里存的值是对应类型的零值
- 只有一个参数。参数是分配的内存空间所存储的变量类型，Go语言里的任何类型都可以是new的参数，比如int， 数组，结构体，甚至函数类型都可以
- 返回的是某类型的指针

**make**

- 分配和初始化内存
- 只能用于slice, map和chan这3个类型，不能用于其它类型。如果是用于slice类型，make函数的第2个参数表示slice的长度，这个参数必须给值
- 返回的是原始类型，也就是slice, map和chan，不是返回指向slice, map和chan的指针

## return 与 defer

return 并不是原子操作, 底层是两个步骤

1. 返回值赋值, 返回值有匿名返回值, 具名返回值
2. 执行defer
3. 执行RET指令, 函数携带当前返回值退出





# REDIS 相关



## Redis过期策略有哪些？

Redis 过期策略是：**定期删除**+**惰性删除**。

**定期删除：**指的是 Redis 默认是每隔 100ms 就随机抽取⼀些设置了过期时间的 key，检查其是否过期，如果过期就删除。

假设 Redis ⾥放了 10w 个 key，都设置了过期时间，你每隔⼏百毫秒，就检查 10w 个 key，那 Redis 基本上就死了，

cpu 负载会很⾼的，消耗在你的检查过期 key 上了。注意，这⾥可不是每隔 100ms 就遍历所有的设置过期时间的

key，那样就是⼀场性能上的**灾难**。实际上 Redis 是每隔 100ms **随机抽取**⼀些 key 来检查和删除的。

**惰性删除：**定期删除可能会导致很多过期 key 到了时间并没有被删除掉，那咋整呢？所以就是惰性删除了。这就是

说，在你获取某个 key 的时候，Redis 会检查⼀下 ，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时

就会删除，不会给你返回任何东⻄。

**定期删除漏掉了很多过期 key，也没有惰性删除，大量key堆积内存咋整？**

答案是：**⾛内存淘汰机制**。

## 内存淘汰机制

Redis 内存淘汰机制有以下⼏个：

**noeviction**: 当内存不⾜以容纳新写⼊数据时，新写⼊操作会报错，这个⼀般没⼈⽤吧，实在是太恶⼼了。

**allkeys-lru**：当内存不⾜以容纳新写⼊数据时，移除最近最少使⽤的 key（这个是**最常⽤**的）。

allkeys-random：当内存不⾜以容纳新写⼊数据时，在**键空间**中，随机移除某个 key，这个⼀般没⼈⽤吧，为啥要随机，肯定是把最近最少使⽤的 key 给⼲掉啊。

volatile-lru：当内存不⾜以容纳新写⼊数据时，在**设置了过期时间的键空间**中，移除最近最少使⽤的 key（这个⼀般不太合适）。

volatile-random：当内存不⾜以容纳新写⼊数据时，在**设置了过期时间的键空间**中，**随机移除**某个 key。

volatile-ttl：当内存不⾜以容纳新写⼊数据时，在**设置了过期时间的键空间**中，有**更早过期时间**的 key 优先移除。



## 持久化的两种⽅式？

RDB：RDB 持久化机制，是对 Redis 中的数据执⾏**周期性**的持久化。

AOF：AOF 机制对每条写⼊命令作为⽇志，以 append-only 的模式写⼊⼀个⽇志⽂件中，在 Redis 重启的时候，可以通过**回放** AOF ⽇志中的写⼊指令来重新构建整个数据集。

通过 RDB 或 AOF，都可以将 Redis 内存中的数据给持久化到磁盘上⾯来，然后可以将这些数据备份到别的地⽅去，AOF**数据更加完整**。

**RDB** **优缺点**

RDB 会⽣成多个数据⽂件，每个数据⽂件都代表了某⼀个时刻中 Redis 的数据，这种多个数据⽂件的⽅式，⾮常适合做冷备

RDB 对 Redis 对外提供的读写服务，影响⾮常⼩，可以让 Redis **保持⾼性能**，因为 Redis 主进程只需要 fork ⼀个⼦进程，让⼦进程执⾏磁盘 IO 操作来进⾏ RDB持久化即可。相对于 AOF 持久化机制来说，直接基于 RDB 数据⽂件来重启和恢复 Redis 进程，更加快速。

RDB恢复数据完整性较AOF缺少上次备份到现在的数据

**AOF** **优缺点**

AOF 可以更好的保护数据不丢失，⼀般 AOF 会每隔 1 秒，通过⼀个后台线程执⾏⼀次 fsync 操作，最多丢失 1秒钟的数据。

AOF ⽇志⽂件以 append-only 模式写⼊，所以没有任何磁盘寻址的开销，写⼊性能⾮常⾼，⽽且⽂件不容易破损，即使⽂件尾部破损，也很容易修复。

AOF ⽇志⽂件即使过⼤的时候，出现后台重写操作，也不会影响客户端的读写。因为在 rewrite log 的时候，会对其中的指令进⾏压缩，创建出⼀份需要恢复数据的最⼩⽇志出来。在创建新⽇志⽂件的时候，⽼的⽇志⽂件还是照常写⼊。当新的 merge 后的⽇志⽂件 ready 的时候，再交换新⽼⽇志⽂件即可。AOF ⽇志⽂件的命令通过可读较强的⽅式进⾏记录，这个特性⾮常**适合做灾难性的误删除的紧急恢复**。⽐如某⼈不⼩⼼⽤ flushall 命令清空了所有数据，只要这个时候后台 rewrite 还没有发⽣，那么就可以⽴即拷⻉ AOF ⽂件，将最后⼀条 flushall 命令给删了，然后再将该 AOF ⽂件放回去，就可以通过恢复机制，⾃动恢复所有数据。

对于同⼀份数据来说，AOF ⽇志⽂件通常⽐ RDB 数据快照⽂件更⼤。AOF 开启后，⽀持的写 QPS 会⽐ RDB ⽀持的写 QPS 低，因为 AOF ⼀般会配置成每秒 fsync ⼀次⽇志⽂件，当然，每秒⼀次 fsync ，性能也还是很⾼的。（如果实时写⼊，那么 QPS 会⼤降，Redis 性能会⼤⼤降低）以前 AOF 发⽣过 bug，就是通过 AOF 记录的⽇志，进⾏数据恢复的时候，没有恢复⼀模⼀样的数据出来。所以说，类似 AOF 这种较为复杂的基于命令⽇志 / merge / 回放的⽅式，⽐基于 RDB 每次持久化⼀份完整的数据快照⽂件的⽅式，更加脆弱⼀些，容易有 bug。不过 AOF 就是为了避免 rewrite 过程导致的 bug，因此每次 rewrite 并不是基于旧的指令⽇志进⾏ merge 的，⽽是**基于当时内存中的数据进⾏指令的重新构建**，这样健壮性会好很多。

**RDB** **和** **AOF** **到底该如何选择**

1. AOF保证数据不丢失
2. RDB做不同程度的冷备AOF ⽂件都丢失或损坏不可⽤的时候，还可以使⽤ RDB 来进⾏快速的数据恢复
3. 建议两个都开启

## Redis Cluster 了解吗？

1. ⾃动将数据进⾏分⽚，每个 master 上放⼀部分数据
2. 提供内置的⾼可⽤⽀持，部分 master 不可⽤时，还是可以继续⼯作的

**分布式寻址算法**

hash 算法（⼤量缓存重建）

⼀致性 hash 算法（⾃动缓存迁移）+ 虚拟节点（⾃动负载均衡）

Redis cluster 的 hash slot 算法

**Redis cluster** **的⾼可⽤与主备切换原理**

Redis cluster 的⾼可⽤的原理，⼏乎跟哨兵是类似的。

**从节点选举**

每个从节点，都根据⾃⼰对 master 复制数据的 offset，来设置⼀个选举时间，offset 越⼤（复制数据越多）的从节点，选举时间越靠前，优先进⾏选举。所有的 master node 开始 slave 选举投票，给要进⾏选举的 slave 进⾏投票，如果⼤部分 master node （N/2 + 1） 都投票给了某个从节点，那么选举通过，那个从节点可以切换成 master。从节点执⾏主备切换，从节点切换为主节点



## Redis 的雪崩、穿透和击穿，如何应对？

**缓存雪崩**

Redis 雪崩"是指在某一时间段，缓存集中失效，导致大量请求直接走数据库，有可能对数据库造成巨大压力，甚至使其宕机，从而使整个服务瘫痪。

解决方案：

- 使用互斥锁
- 缓存的失效时间再加上一个随机值
- Redis cluster，避免全盘崩溃

**缓存穿透**

主要指查询的数据在缓存和数据库中都不存在的情况。在这种情况下，客户端仍然会不断地发起请求，导致每次请求都会压向数据库

- 将此key对应的value设置为一个默认的值，比如设置为空(NULL)，并设置一个缓存的失效时间
- 布隆过滤器
- 使用互斥锁

**缓存击穿**

缓存击穿是指热点key在某个时间点过期的时候，而恰好在这个时间点对这个Key有大量的并发请求过来，从而大量的请求打到db。

- 若缓存的数据是基本不会发⽣更新的，则可尝试将该热点数据设置为永不过期。

- 使用互斥锁

  

## redis 如何实现延迟队列？

利用有序集合的 score 属性，将时间戳设置到该属性上，然后定时的对其排序，查看最近要执行的记录，如果时间到了，则取出来消费后删除，即可达到延迟队列的目的。

## 秒杀系统设计

- 业务特点：
  1、瞬时并发量大，秒杀时会有大量用户在同一时间进行抢购，瞬时并发访问量突增几倍、甚至几十倍以上
  2、库存量少，一般秒杀活动商品量很少，这就导致了只有极少量用户能成功购买到。
  3、业务和流程较为简单，一般都是下订单、扣库存、支付订单。
- 技术难点：
  1、若秒杀活动若与其他营销活动同时进行，可能会对其他活动造成冲击，极端情况下可能导致整个服务宕机。
  2、页面流量突增，秒杀活动用户访问量会突增。需确保访问量的突增不会对服务器、数据库、Redis等造成过大的压力。
  3、秒杀活动库存量小，瞬时下单量大，易造成超卖现象
- 架构设计思想
  1、限流：由于库存量很少，对应的只有少部分用户才能秒杀成功。所以要限制大部分用户流量，只准少量用户流量进入后端服务器。
  2、削峰：秒杀开始瞬间，大量用户进来会有一个瞬间流量峰值。把瞬间峰值变得更平缓是设计好秒杀系统关键因素。一般的采用缓存和MQ实现流量的削峰填谷。
  3、异步：秒杀可以当做高并发系统处理。即可以从业务上考虑，将同步的业务，设计成异步处理的任务。
  4、缓存：秒杀瓶颈主要体现在下单、扣库存的数据操作中。关系型数据库写入和读取效率较低。若将部分操作放到缓存中能极大提高并发效率(如使用Redis操作库存)
- 客户端优化
  1、秒杀页面：
  如果秒杀页面的资源，如：CSS、JS、图片、商品详情等都经后端，服务肯定承受不住。如果将这个页面进行静态化，秒杀时肯定能起到压力分散的作用。
  2、防止提前下单：
  使用JS控制提交订单按钮，如果秒杀时间，就不能点击该按钮。
- 服务端优化
  1、对查询秒杀商品进行优化
  将首次查询到的商品信息进行数据放入缓存，后面再访问时直接返回缓存的信息。
  2、对库存的优化
  在设置秒杀活动时就将商品库存放于Redis中，在下单扣库存时，直接对Redis进行操作。
  3、后端流量控制优化（参加用户量过大时）
  使用消息队列、异步处理等方式解决。即超过系统水位线的请求直接拒绝掉。
- 核心思想：
  1、层层过滤，逐渐递减瞬时访问，降低下游的压力，减少最终对数据库的冲击
  2、充分利用缓存与消息队列，提高请求处理速度以及削峰填谷的作用
